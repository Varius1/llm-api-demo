# LLM CLI Chat

Терминальный клиент для [OpenRouter API](https://openrouter.ai/) с красивым выводом через Rich, бенчмарком моделей и моделью-судьёй.

## Требования

- Python 3.11+
- API ключ [OpenRouter](https://openrouter.ai/keys)

## Установка

```bash
# Клонировать репозиторий
git clone https://github.com/your-user/llm-api-demo.git
cd llm-api-demo

# Создать виртуальное окружение и установить зависимости
python -m venv .venv

# Linux / macOS
source .venv/bin/activate

# Windows
.venv\Scripts\activate

pip install -e .
```

## Запуск

```bash
# Интерактивный чат (при первом запуске попросит API-ключ)
python -m llm_cli

# Или через установленную команду
llm-cli

# Через переменную окружения
export OPENROUTER_API_KEY="sk-or-v1-ваш-ключ"   # Linux/macOS
set OPENROUTER_API_KEY=sk-or-v1-ваш-ключ         # Windows CMD
python -m llm_cli
```

### Бенчмарк моделей

```bash
# Сравнить модели с промптом по умолчанию
python -m llm_cli --compare

# Свой промпт и температура
python -m llm_cli --compare --prompt "Что такое рекурсия?" --temp 0.5
```

## Команды в чате

| Команда | Описание |
|---|---|
| `/strategy sliding` | Стратегия Sliding Window: последние N сообщений, старое отбрасывается |
| `/strategy sliding 15` | Sliding Window с keep_n=15 |
| `/strategy facts` | Стратегия Sticky Facts: KV-память фактов + последние N сообщений |
| `/strategy summary` | Стратегия Summary: эвристическое сжатие старой части (исходная) |
| `/strategy branch` | Стратегия Branching: независимые ветки диалога |
| `/branch save [имя]` | Сохранить checkpoint текущей истории как ветку (по умолчанию `main`) |
| `/branch switch <имя>` | Переключиться на сохранённую ветку |
| `/branch list` | Список всех сохранённых веток |
| `/facts` | Показать текущую KV-память (только для стратегии `facts`) |
| `/demo-strategies` | Автопрогон одного сценария для всех 3 стратегий + сравнительная таблица |
| `/demo-compare` | Автопрогон: сравнение без сжатия vs со сжатием (summary) |
| `/compress on` / `/compress off` | Переключить summary-компрессию (псевдоним для `/strategy summary|sliding`) |
| `/temp 0.7` | Установить температуру генерации |
| `/model google/gemma-2-9b-it` | Сменить модель |
| `/overflow 9000` | Отправить большой тестовый запрос в strict-режиме (`transforms: []`) |
| `/clear` | Очистить историю диалога (в памяти и в файле истории) |
| `exit` / `quit` | Выйти |

Ввод сообщения: напишите текст и нажмите Enter дважды для отправки.

## Конфигурация

При первом запуске создаётся конфиг-файл:

- **Linux/macOS:** `~/.config/llm-cli/config.toml`
- **Windows:** `%APPDATA%\llm-cli\config.toml`

История диалога сохраняется отдельно:

- **Linux/macOS:** `~/.config/llm-cli/history.json`
- **Windows:** `%APPDATA%\llm-cli\history.json`

При новом запуске история автоматически загружается, и диалог продолжается с прошлого контекста.

```toml
[general]
api_key = "sk-or-v1-..."
default_model = "google/gemma-2-9b-it"
temperature = 0.2
benchmark_prompt = ""

[[models]]
id = "meta-llama/llama-3.3-70b-instruct"
tier = "Слабая (дешёвая)"
display_name = "Llama 3.3 70B"
input_price_per_million = 0.1
output_price_per_million = 0.32
url = "https://openrouter.ai/meta-llama/llama-3.3-70b-instruct"
```

## Стратегии управления контекстом

Агент поддерживает 3 стратегии управления контекстом (переключение командой `/strategy`):

### Стратегия 1: Sliding Window (`/strategy sliding`)

- В запрос попадают только **последние N сообщений** (по умолчанию N=10).
- Всё более старое молча отбрасывается.
- Плюс: самое дешёвое по токенам.
- Минус: модель не помнит ничего старше N сообщений.

```
/strategy sliding       → keep_n=10 (по умолчанию)
/strategy sliding 15    → keep_n=15
```

### Стратегия 2: Sticky Facts (`/strategy facts`)

- Поддерживается **KV-словарь фактов** (ключ → значение).
- После каждого сообщения пользователя эвристически извлекаются факты:
  `проект`, `клиент`, `дедлайн`, `бюджет`, `команда`, `стек`, `риски`, `ограничения`, `требования`, `kpi` и другие.
- В запрос отправляется: `system + блок фактов + последние N сообщений`.
- Команда `/facts` — показать текущий словарь.
- Плюс: ключевые данные не теряются даже в длинном диалоге.
- Минус: детали формулировок теряются, остаются только извлечённые поля.

### Стратегия 3: Branching (`/strategy branch`)

- Позволяет сохранять **независимые ветки** от одной точки диалога.
- `/branch save main` — сохранить snapshot текущей истории как ветку `main`.
- `/branch save alt` — сохранить другую ветку `alt`.
- `/branch switch alt` — переключиться на ветку `alt`, история заменяется.
- `/branch list` — таблица всех веток (имя, дата, число сообщений, активная).
- Каждая ветка хранится независимо; переключение не теряет другие ветки.

```
/strategy branch
/branch save main           → checkpoint как ветка «main»
/branch save alt            → тот же checkpoint как ветка «alt»
/branch switch alt          → переходим в ветку «alt»
... диалог про альтернативный подход ...
/branch switch main         → возвращаемся в «main»
/branch list                → смотрим все ветки
```

### Сравнение стратегий

Команда `/demo-strategies` прогоняет один и тот же тестовый сценарий (Atlas CRM, 28 фактов + 3 контрольных вопроса) поочерёдно для Sliding Window, Sticky Facts и Summary и выводит сравнительную таблицу качества/токенов.

### Summary (исходная стратегия, `/strategy summary`)

- Последние `N=10` сообщений отправляются «как есть».
- Более старая часть заменяется структурированным эвристическим `summary` (ключевые факты).
- Summary пересчитывается каждые `10` новых архивных сообщений.
- Компрессия включается только при диалогах от `25` сообщений.
- Псевдоним: `/compress on` = `/strategy summary`, `/compress off` = `/strategy sliding`.

## Токены и стоимость в чате

После каждого ответа CLI показывает:

- токены текущего запроса (оценка),
- токены всей истории (`prompt`, по данным API),
- токены ответа модели (`response`, по данным API),
- итог токенов (`total`) и стоимость за ход/сессию.

Это позволяет увидеть, как при длинном диалоге растут `prompt_tokens` и стоимость.
В режиме компрессии дополнительно показывается оценка контекста до/после сжатия.

## Демонстрация переполнения контекста

Для задания можно воспроизвести реальное переполнение лимита модели:

```bash
python -m llm_cli
```

В чате:

1. `/model google/gemma-2-9b-it` (контекст ~8192 токена)
2. `/clear`
3. `/overflow 9000` (команда отправляется в strict-режиме с `transforms: []`)

Ожидаемое поведение: API вернёт `HTTP 400` с ошибкой переполнения контекста.

## Как сравнить «без сжатия» и «со сжатием»

1. Очистить историю: `/clear`.
2. Выключить компрессию: `/compress off`.
3. Прогнать одинаковый сценарий диалога и зафиксировать:
   - качество ответов на финальные контрольные вопросы,
   - `prompt/response/total` токены и стоимость.
4. Снова `/clear`, включить компрессию: `/compress on`.
5. Повторить тот же сценарий и сравнить качество и расход токенов/стоимости.

Быстрый вариант: команда `/demo-compare` сама прогонит одинаковый сценарий в двух режимах
(`compress off` и `compress on`) и выведет итоговую таблицу + краткое сравнение ответов.

## Архитектура

```
CLI (chat.py)  →  Agent (agent.py)  →  HTTP-клиент (api.py)  →  OpenRouter API
   ввод/вывод      логика + история      транспорт               LLM
                       ↓
                  strategy.py  — стратегии управления контекстом
```

Агент (`Agent`) — отдельная сущность, инкапсулирующая логику диалога с LLM:
- хранит историю сообщений (контекст диалога)
- управляет параметрами генерации (модель, температура)
- поддерживает 4 стратегии управления контекстом с переключением на лету
- не зависит от интерфейса — можно подключить к CLI, веб-серверу или боту

## Структура проекта

```
src/llm_cli/
  __init__.py       — пакет
  __main__.py       — точка входа, argparse
  agent.py          — LLM-агент (история диалога, 4 стратегии, ветки)
  api.py            — HTTP-клиент OpenRouter (httpx)
  benchmark.py      — бенчмарк моделей + модель-судья
  chat.py           — интерактивный чат-цикл (тонкая обёртка над агентом)
  config.py         — загрузка/сохранение TOML-конфига
  display.py        — Rich-отрисовка (таблицы, панели, спиннеры)
  models.py         — Pydantic-модели данных
  strategy.py       — стратегии управления контекстом (Sliding Window, Sticky Facts)
```
